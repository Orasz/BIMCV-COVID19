{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "customUNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/Orasz/CNN4COVID19/blob/main/customUNetDice04.ipynb",
      "authorship_tag": "ABX9TyOyaRYFHkcBFt4RV66lRfWx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Orasz/CNN4COVID19/blob/main/tmp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toywndUptxNa",
        "outputId": "63a48b93-ae83-427b-89a1-c6d67ce8e218"
      },
      "source": [
        "!pip install git+https://github.com/albumentations-team/albumentations.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/albumentations-team/albumentations.git\n",
            "  Cloning https://github.com/albumentations-team/albumentations.git to /tmp/pip-req-build-kvrm89k6\n",
            "  Running command git clone -q https://github.com/albumentations-team/albumentations.git /tmp/pip-req-build-kvrm89k6\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (0.16.2)\n",
            "Collecting imgaug>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n",
            "\u001b[K     |████████████████████████████████| 952kB 16.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.5.2) (4.1.2.30)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (2.5)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (2.4.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (7.0.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (1.1.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (3.2.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.5.2) (1.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.5.2) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations==0.5.2) (4.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==0.5.2) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==0.5.2) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==0.5.2) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==0.5.2) (1.3.1)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.5.2-cp37-none-any.whl size=86173 sha256=826a0ef1966070cb3ed8c47ea293560dfbd95439353fc087971cbc4f148ed06b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qi0p_4qp/wheels/e2/85/3e/2a40fac5cc1f43ced656603bb2fca1327b30ec7de1b1b66517\n",
            "Successfully built albumentations\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.5.2 imgaug-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOZF3gqPJy8x",
        "outputId": "92cd4990-b172-4400-b8c1-3512b542f7dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\r\n",
        "import os\r\n",
        "from PIL import Image\r\n",
        "from torch.utils.data import Dataset\r\n",
        "import numpy as np\r\n",
        "import albumentations as A\r\n",
        "from albumentations.pytorch import ToTensorV2\r\n",
        "from tqdm import tqdm\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "\r\n",
        "#Building the DATASET class\r\n",
        "\r\n",
        "\r\n",
        "class QaTaDataset(Dataset):\r\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\r\n",
        "        self.image_dir = image_dir\r\n",
        "        self.mask_dir = mask_dir\r\n",
        "        self.transform = transform\r\n",
        "        self.images = os.listdir(image_dir)\r\n",
        "        self.masks = os.listdir(mask_dir)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.images)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\r\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[index])\r\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\r\n",
        "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\r\n",
        "        mask[mask == 255.0] = 1.0\r\n",
        "\r\n",
        "        if self.transform is not None:\r\n",
        "            augmentations = self.transform(image=image, mask=mask)\r\n",
        "            image = augmentations[\"image\"]\r\n",
        "            mask = augmentations[\"mask\"]\r\n",
        "\r\n",
        "        return image, mask\r\n",
        "print(\"done\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8beJ2i_jenTc",
        "outputId": "e77562a1-ea03-457e-ae6a-9e1edf1cbda7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#UTILS\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "def save_checkpoint(state, filename=\"/content/drive/MyDrive/UNET_checkpoint.pth.tar\"):\r\n",
        "    print(\"=> Saving checkpoint\")\r\n",
        "    torch.save(state, filename)\r\n",
        "\r\n",
        "def load_checkpoint(checkpoint, model):\r\n",
        "    print(\"=> Loading checkpoint\")\r\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\r\n",
        "\r\n",
        "def get_loaders(\r\n",
        "    train_dir,\r\n",
        "    train_maskdir,\r\n",
        "    val_dir,\r\n",
        "    val_maskdir,\r\n",
        "    batch_size,\r\n",
        "    train_transform,\r\n",
        "    val_transform,\r\n",
        "    num_workers=4,\r\n",
        "    pin_memory=True,\r\n",
        "):\r\n",
        "    train_ds = QaTaDataset(\r\n",
        "        image_dir=train_dir,\r\n",
        "        mask_dir=train_maskdir,\r\n",
        "        transform=train_transform,\r\n",
        "    )\r\n",
        "\r\n",
        "    train_loader = DataLoader(\r\n",
        "        train_ds,\r\n",
        "        batch_size=batch_size,\r\n",
        "        num_workers=num_workers,\r\n",
        "        pin_memory=pin_memory,\r\n",
        "        shuffle=True,\r\n",
        "    )\r\n",
        "\r\n",
        "    val_ds = QaTaDataset(\r\n",
        "        image_dir=val_dir,\r\n",
        "        mask_dir=val_maskdir,\r\n",
        "        transform=val_transform,\r\n",
        "    )\r\n",
        "\r\n",
        "    val_loader = DataLoader(\r\n",
        "        val_ds,\r\n",
        "        batch_size=batch_size,\r\n",
        "        num_workers=num_workers,\r\n",
        "        pin_memory=pin_memory,\r\n",
        "        shuffle=False,\r\n",
        "    )\r\n",
        "\r\n",
        "    return train_loader, val_loader\r\n",
        "\r\n",
        "def check_accuracy(loader, model, device=\"cuda\"):\r\n",
        "    num_correct = 0\r\n",
        "    num_pixels = 0\r\n",
        "    dice_score = 0\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        for x, y in loader:\r\n",
        "            x = x.to(device)\r\n",
        "            y = y.to(device).unsqueeze(1)\r\n",
        "            preds = torch.sigmoid(model(x))\r\n",
        "            preds = (preds > 0.5).float()\r\n",
        "            num_correct += (preds == y).sum()\r\n",
        "            num_pixels += torch.numel(preds)\r\n",
        "            dice_score += (2 * (preds * y).sum()) / (\r\n",
        "                (preds + y).sum() + 1e-8\r\n",
        "            )\r\n",
        "\r\n",
        "    print(\r\n",
        "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\r\n",
        "    )\r\n",
        "    print(f\"Dice score: {dice_score/len(loader)}\")\r\n",
        "    model.train()\r\n",
        "\r\n",
        "def save_predictions_as_imgs(\r\n",
        "    loader, model, folder=\"saved_images/\", device=\"cuda\"\r\n",
        "):\r\n",
        "    model.eval()\r\n",
        "    for idx, (x, y) in enumerate(loader):\r\n",
        "        x = x.to(device=device)\r\n",
        "        with torch.no_grad():\r\n",
        "            preds = torch.sigmoid(model(x))\r\n",
        "            preds = (preds > 0.5).float()\r\n",
        "        torchvision.utils.save_image(\r\n",
        "            preds, f\"{folder}/pred_{idx}.png\"\r\n",
        "        )\r\n",
        "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\r\n",
        "\r\n",
        "    model.train()\r\n",
        "print(\"done\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRGnyl5VMFbM"
      },
      "source": [
        "from torch import nn as nn\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "@torch.jit.script\r\n",
        "def autocrop(encoder_layer: torch.Tensor, decoder_layer: torch.Tensor):\r\n",
        "    \"\"\"\r\n",
        "    Center-crops the encoder_layer to the size of the decoder_layer,\r\n",
        "    so that merging (concatenation) between levels/blocks is possible.\r\n",
        "    This is only necessary for input sizes != 2**n for 'same' padding and always required for 'valid' padding.\r\n",
        "    \"\"\"\r\n",
        "    if encoder_layer.shape[2:] != decoder_layer.shape[2:]:\r\n",
        "        ds = encoder_layer.shape[2:]\r\n",
        "        es = decoder_layer.shape[2:]\r\n",
        "        assert ds[0] >= es[0]\r\n",
        "        assert ds[1] >= es[1]\r\n",
        "        if encoder_layer.dim() == 4:  # 2D\r\n",
        "            encoder_layer = encoder_layer[\r\n",
        "                            :,\r\n",
        "                            :,\r\n",
        "                            ((ds[0] - es[0]) // 2):((ds[0] + es[0]) // 2),\r\n",
        "                            ((ds[1] - es[1]) // 2):((ds[1] + es[1]) // 2)\r\n",
        "                            ]\r\n",
        "        elif encoder_layer.dim() == 5:  # 3D\r\n",
        "            assert ds[2] >= es[2]\r\n",
        "            encoder_layer = encoder_layer[\r\n",
        "                            :,\r\n",
        "                            :,\r\n",
        "                            ((ds[0] - es[0]) // 2):((ds[0] + es[0]) // 2),\r\n",
        "                            ((ds[1] - es[1]) // 2):((ds[1] + es[1]) // 2),\r\n",
        "                            ((ds[2] - es[2]) // 2):((ds[2] + es[2]) // 2),\r\n",
        "                            ]\r\n",
        "    return encoder_layer, decoder_layer\r\n",
        "\r\n",
        "\r\n",
        "def conv_layer(dim: int):\r\n",
        "    if dim == 3:\r\n",
        "        return nn.Conv3d\r\n",
        "    elif dim == 2:\r\n",
        "        return nn.Conv2d\r\n",
        "\r\n",
        "\r\n",
        "def get_conv_layer(in_channels: int,\r\n",
        "                   out_channels: int,\r\n",
        "                   kernel_size: int = 3,\r\n",
        "                   stride: int = 1,\r\n",
        "                   padding: int = 1,\r\n",
        "                   bias: bool = True,\r\n",
        "                   dim: int = 2):\r\n",
        "    return conv_layer(dim)(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\r\n",
        "                           bias=bias)\r\n",
        "\r\n",
        "\r\n",
        "def conv_transpose_layer(dim: int):\r\n",
        "    if dim == 3:\r\n",
        "        return nn.ConvTranspose3d\r\n",
        "    elif dim == 2:\r\n",
        "        return nn.ConvTranspose2d\r\n",
        "\r\n",
        "\r\n",
        "def get_up_layer(in_channels: int,\r\n",
        "                 out_channels: int,\r\n",
        "                 kernel_size: int = 2,\r\n",
        "                 stride: int = 2,\r\n",
        "                 dim: int = 3,\r\n",
        "                 up_mode: str = 'transposed',\r\n",
        "                 ):\r\n",
        "    if up_mode == 'transposed':\r\n",
        "        return conv_transpose_layer(dim)(in_channels, out_channels, kernel_size=kernel_size, stride=stride)\r\n",
        "    else:\r\n",
        "        return nn.Upsample(scale_factor=2.0, mode=up_mode)\r\n",
        "\r\n",
        "\r\n",
        "def maxpool_layer(dim: int):\r\n",
        "    if dim == 3:\r\n",
        "        return nn.MaxPool3d\r\n",
        "    elif dim == 2:\r\n",
        "        return nn.MaxPool2d\r\n",
        "\r\n",
        "\r\n",
        "def get_maxpool_layer(kernel_size: int = 2,\r\n",
        "                      stride: int = 2,\r\n",
        "                      padding: int = 0,\r\n",
        "                      dim: int = 2):\r\n",
        "    return maxpool_layer(dim=dim)(kernel_size=kernel_size, stride=stride, padding=padding)\r\n",
        "\r\n",
        "\r\n",
        "def get_activation(activation: str):\r\n",
        "    if activation == 'relu':\r\n",
        "        return nn.ReLU()\r\n",
        "    elif activation == 'leaky':\r\n",
        "        return nn.LeakyReLU(negative_slope=0.1)\r\n",
        "    elif activation == 'elu':\r\n",
        "        return nn.ELU()\r\n",
        "\r\n",
        "\r\n",
        "def get_normalization(normalization: str,\r\n",
        "                      num_channels: int,\r\n",
        "                      dim: int):\r\n",
        "    if normalization == 'batch':\r\n",
        "        if dim == 3:\r\n",
        "            return nn.BatchNorm3d(num_channels)\r\n",
        "        elif dim == 2:\r\n",
        "            return nn.BatchNorm2d(num_channels)\r\n",
        "    elif normalization == 'instance':\r\n",
        "        if dim == 3:\r\n",
        "            return nn.InstanceNorm3d(num_channels)\r\n",
        "        elif dim == 2:\r\n",
        "            return nn.InstanceNorm2d(num_channels)\r\n",
        "    elif 'group' in normalization:\r\n",
        "        num_groups = int(normalization.partition('group')[-1])  # get the group size from string\r\n",
        "        return nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)\r\n",
        "\r\n",
        "\r\n",
        "class Concatenate(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Concatenate, self).__init__()\r\n",
        "\r\n",
        "    def forward(self, layer_1, layer_2):\r\n",
        "        x = torch.cat((layer_1, layer_2), 1)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class DownBlock(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    A helper Module that performs 2 Convolutions and 1 MaxPool.\r\n",
        "    An activation follows each convolution.\r\n",
        "    A normalization layer follows each convolution.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 in_channels: int,\r\n",
        "                 out_channels: int,\r\n",
        "                 pooling: bool = True,\r\n",
        "                 activation: str = 'relu',\r\n",
        "                 normalization: str = None,\r\n",
        "                 dim: str = 2,\r\n",
        "                 conv_mode: str = 'same'):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.in_channels = in_channels\r\n",
        "        self.out_channels = out_channels\r\n",
        "        self.pooling = pooling\r\n",
        "        self.normalization = normalization\r\n",
        "        if conv_mode == 'same':\r\n",
        "            self.padding = 1\r\n",
        "        elif conv_mode == 'valid':\r\n",
        "            self.padding = 0\r\n",
        "        self.dim = dim\r\n",
        "        self.activation = activation\r\n",
        "\r\n",
        "        # conv layers\r\n",
        "        self.conv1 = get_conv_layer(self.in_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\r\n",
        "                                    bias=True, dim=self.dim)\r\n",
        "        self.conv2 = get_conv_layer(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\r\n",
        "                                    bias=True, dim=self.dim)\r\n",
        "\r\n",
        "        # pooling layer\r\n",
        "        if self.pooling:\r\n",
        "            self.pool = get_maxpool_layer(kernel_size=2, stride=2, padding=0, dim=self.dim)\r\n",
        "\r\n",
        "        # activation layers\r\n",
        "        self.act1 = get_activation(self.activation)\r\n",
        "        self.act2 = get_activation(self.activation)\r\n",
        "\r\n",
        "        # normalization layers\r\n",
        "        if self.normalization:\r\n",
        "            self.norm1 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\r\n",
        "                                           dim=self.dim)\r\n",
        "            self.norm2 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\r\n",
        "                                           dim=self.dim)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        y = self.conv1(x)  # convolution 1\r\n",
        "        y = self.act1(y)  # activation 1\r\n",
        "        if self.normalization:\r\n",
        "            y = self.norm1(y)  # normalization 1\r\n",
        "        y = self.conv2(y)  # convolution 2\r\n",
        "        y = self.act2(y)  # activation 2\r\n",
        "        if self.normalization:\r\n",
        "            y = self.norm2(y)  # normalization 2\r\n",
        "\r\n",
        "        before_pooling = y  # save the outputs before the pooling operation\r\n",
        "        if self.pooling:\r\n",
        "            y = self.pool(y)  # pooling\r\n",
        "        return y, before_pooling\r\n",
        "\r\n",
        "\r\n",
        "class UpBlock(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    A helper Module that performs 2 Convolutions and 1 UpConvolution/Upsample.\r\n",
        "    An activation follows each convolution.\r\n",
        "    A normalization layer follows each convolution.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 in_channels: int,\r\n",
        "                 out_channels: int,\r\n",
        "                 activation: str = 'relu',\r\n",
        "                 normalization: str = None,\r\n",
        "                 dim: int = 3,\r\n",
        "                 conv_mode: str = 'same',\r\n",
        "                 up_mode: str = 'transposed'\r\n",
        "                 ):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.in_channels = in_channels\r\n",
        "        self.out_channels = out_channels\r\n",
        "        self.normalization = normalization\r\n",
        "        if conv_mode == 'same':\r\n",
        "            self.padding = 1\r\n",
        "        elif conv_mode == 'valid':\r\n",
        "            self.padding = 0\r\n",
        "        self.dim = dim\r\n",
        "        self.activation = activation\r\n",
        "        self.up_mode = up_mode\r\n",
        "\r\n",
        "        # upconvolution/upsample layer\r\n",
        "        self.up = get_up_layer(self.in_channels, self.out_channels, kernel_size=2, stride=2, dim=self.dim,\r\n",
        "                               up_mode=self.up_mode)\r\n",
        "\r\n",
        "        # conv layers\r\n",
        "        self.conv0 = get_conv_layer(self.in_channels, self.out_channels, kernel_size=1, stride=1, padding=0,\r\n",
        "                                    bias=True, dim=self.dim)\r\n",
        "        self.conv1 = get_conv_layer(2 * self.out_channels, self.out_channels, kernel_size=3, stride=1,\r\n",
        "                                    padding=self.padding,\r\n",
        "                                    bias=True, dim=self.dim)\r\n",
        "        self.conv2 = get_conv_layer(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\r\n",
        "                                    bias=True, dim=self.dim)\r\n",
        "\r\n",
        "        # activation layers\r\n",
        "        self.act0 = get_activation(self.activation)\r\n",
        "        self.act1 = get_activation(self.activation)\r\n",
        "        self.act2 = get_activation(self.activation)\r\n",
        "\r\n",
        "        # normalization layers\r\n",
        "        if self.normalization:\r\n",
        "            self.norm0 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\r\n",
        "                                           dim=self.dim)\r\n",
        "            self.norm1 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\r\n",
        "                                           dim=self.dim)\r\n",
        "            self.norm2 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\r\n",
        "                                           dim=self.dim)\r\n",
        "\r\n",
        "        # concatenate layer\r\n",
        "        self.concat = Concatenate()\r\n",
        "\r\n",
        "    def forward(self, encoder_layer, decoder_layer):\r\n",
        "        \"\"\" Forward pass\r\n",
        "        Arguments:\r\n",
        "            encoder_layer: Tensor from the encoder pathway\r\n",
        "            decoder_layer: Tensor from the decoder pathway (to be up'd)\r\n",
        "        \"\"\"\r\n",
        "        up_layer = self.up(decoder_layer)  # up-convolution/up-sampling\r\n",
        "        cropped_encoder_layer, dec_layer = autocrop(encoder_layer, up_layer)  # cropping\r\n",
        "\r\n",
        "        if self.up_mode != 'transposed':\r\n",
        "            # We need to reduce the channel dimension with a conv layer\r\n",
        "            up_layer = self.conv0(up_layer)  # convolution 0\r\n",
        "        up_layer = self.act0(up_layer)  # activation 0\r\n",
        "        if self.normalization:\r\n",
        "            up_layer = self.norm0(up_layer)  # normalization 0\r\n",
        "\r\n",
        "        merged_layer = self.concat(up_layer, cropped_encoder_layer)  # concatenation\r\n",
        "        y = self.conv1(merged_layer)  # convolution 1\r\n",
        "        y = self.act1(y)  # activation 1\r\n",
        "        if self.normalization:\r\n",
        "            y = self.norm1(y)  # normalization 1\r\n",
        "        y = self.conv2(y)  # convolution 2\r\n",
        "        y = self.act2(y)  # acivation 2\r\n",
        "        if self.normalization:\r\n",
        "            y = self.norm2(y)  # normalization 2\r\n",
        "        return y\r\n",
        "\r\n",
        "\r\n",
        "class UNet(nn.Module):\r\n",
        "    def __init__(self,\r\n",
        "                 in_channels: int = 1,\r\n",
        "                 out_channels: int = 2,\r\n",
        "                 n_blocks: int = 4,\r\n",
        "                 start_filters: int = 32,\r\n",
        "                 activation: str = 'relu',\r\n",
        "                 normalization: str = 'batch',\r\n",
        "                 conv_mode: str = 'same',\r\n",
        "                 dim: int = 2,\r\n",
        "                 up_mode: str = 'transposed'\r\n",
        "                 ):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.in_channels = in_channels\r\n",
        "        self.out_channels = out_channels\r\n",
        "        self.n_blocks = n_blocks\r\n",
        "        self.start_filters = start_filters\r\n",
        "        self.activation = activation\r\n",
        "        self.normalization = normalization\r\n",
        "        self.conv_mode = conv_mode\r\n",
        "        self.dim = dim\r\n",
        "        self.up_mode = up_mode\r\n",
        "\r\n",
        "        self.down_blocks = []\r\n",
        "        self.up_blocks = []\r\n",
        "\r\n",
        "        # create encoder path\r\n",
        "        for i in range(self.n_blocks):\r\n",
        "            num_filters_in = self.in_channels if i == 0 else num_filters_out\r\n",
        "            num_filters_out = self.start_filters * (2 ** i)\r\n",
        "            pooling = True if i < self.n_blocks - 1 else False\r\n",
        "\r\n",
        "            down_block = DownBlock(in_channels=num_filters_in,\r\n",
        "                                   out_channels=num_filters_out,\r\n",
        "                                   pooling=pooling,\r\n",
        "                                   activation=self.activation,\r\n",
        "                                   normalization=self.normalization,\r\n",
        "                                   conv_mode=self.conv_mode,\r\n",
        "                                   dim=self.dim)\r\n",
        "\r\n",
        "            self.down_blocks.append(down_block)\r\n",
        "\r\n",
        "        # create decoder path (requires only n_blocks-1 blocks)\r\n",
        "        for i in range(n_blocks - 1):\r\n",
        "            num_filters_in = num_filters_out\r\n",
        "            num_filters_out = num_filters_in // 2\r\n",
        "\r\n",
        "            up_block = UpBlock(in_channels=num_filters_in,\r\n",
        "                               out_channels=num_filters_out,\r\n",
        "                               activation=self.activation,\r\n",
        "                               normalization=self.normalization,\r\n",
        "                               conv_mode=self.conv_mode,\r\n",
        "                               dim=self.dim,\r\n",
        "                               up_mode=self.up_mode)\r\n",
        "\r\n",
        "            self.up_blocks.append(up_block)\r\n",
        "\r\n",
        "        # final convolution\r\n",
        "        self.conv_final = get_conv_layer(num_filters_out, self.out_channels, kernel_size=1, stride=1, padding=0,\r\n",
        "                                         bias=True, dim=self.dim)\r\n",
        "\r\n",
        "        # add the list of modules to current module\r\n",
        "        self.down_blocks = nn.ModuleList(self.down_blocks)\r\n",
        "        self.up_blocks = nn.ModuleList(self.up_blocks)\r\n",
        "\r\n",
        "        # initialize the weights\r\n",
        "        self.initialize_parameters()\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def weight_init(module, method, **kwargs):\r\n",
        "        if isinstance(module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d)):\r\n",
        "            method(module.weight, **kwargs)  # weights\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def bias_init(module, method, **kwargs):\r\n",
        "        if isinstance(module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d)):\r\n",
        "            method(module.bias, **kwargs)  # bias\r\n",
        "\r\n",
        "    def initialize_parameters(self,\r\n",
        "                              method_weights=nn.init.xavier_uniform_,\r\n",
        "                              method_bias=nn.init.zeros_,\r\n",
        "                              kwargs_weights={},\r\n",
        "                              kwargs_bias={}\r\n",
        "                              ):\r\n",
        "        for module in self.modules():\r\n",
        "            self.weight_init(module, method_weights, **kwargs_weights)  # initialize weights\r\n",
        "            self.bias_init(module, method_bias, **kwargs_bias)  # initialize bias\r\n",
        "\r\n",
        "    def forward(self, x: torch.tensor):\r\n",
        "        encoder_output = []\r\n",
        "\r\n",
        "        # Encoder pathway\r\n",
        "        for module in self.down_blocks:\r\n",
        "            x, before_pooling = module(x)\r\n",
        "            encoder_output.append(before_pooling)\r\n",
        "\r\n",
        "        # Decoder pathway\r\n",
        "        for i, module in enumerate(self.up_blocks):\r\n",
        "            before_pool = encoder_output[-(i + 2)]\r\n",
        "            x = module(before_pool, x)\r\n",
        "\r\n",
        "        x = self.conv_final(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        attributes = {attr_key: self.__dict__[attr_key] for attr_key in self.__dict__.keys() if '_' not in attr_key[0] and 'training' not in attr_key}\r\n",
        "        d = {self.__class__.__name__: attributes}\r\n",
        "        return f'{d}'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVeJU1z9daJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a81ee6a1-4011-4da9-d698-068db8dad6ec"
      },
      "source": [
        "#TRAINING\r\n",
        "# Hyperparameters etc.\r\n",
        "LEARNING_RATE = 1e-4\r\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
        "BATCH_SIZE = 16\r\n",
        "NUM_EPOCHS = 4\r\n",
        "NUM_WORKERS = 2\r\n",
        "IMAGE_HEIGHT = 256  \r\n",
        "IMAGE_WIDTH = 256 \r\n",
        "PIN_MEMORY = True\r\n",
        "LOAD_MODEL = False\r\n",
        "TRAIN_IMG_DIR = \"/content/drive/MyDrive/UNET/train_img/\"\r\n",
        "TRAIN_MASK_DIR = \"/content/drive/MyDrive/UNET/train_masks/\"\r\n",
        "VAL_IMG_DIR = \"/content/drive/MyDrive/UNET/val_img/\"\r\n",
        "VAL_MASK_DIR = \"/content/drive/MyDrive/UNET/val_masks/\"\r\n",
        "\r\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\r\n",
        "    loop = tqdm(loader)\r\n",
        "\r\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\r\n",
        "        data = data.to(device=DEVICE)\r\n",
        "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\r\n",
        "\r\n",
        "        # forward\r\n",
        "        with torch.cuda.amp.autocast():\r\n",
        "            predictions = model(data)\r\n",
        "            loss = loss_fn(predictions, targets)\r\n",
        "\r\n",
        "        # backward\r\n",
        "        optimizer.zero_grad()\r\n",
        "        scaler.scale(loss).backward()\r\n",
        "        scaler.step(optimizer)\r\n",
        "        scaler.update()\r\n",
        "\r\n",
        "        # update tqdm loop\r\n",
        "        loop.set_postfix(loss=loss.item())\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "train_transform = A.Compose(\r\n",
        "        [\r\n",
        "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\r\n",
        "            A.CenterCrop(224,224),\r\n",
        "            A.Rotate(limit=35, p=1.0),\r\n",
        "            A.HorizontalFlip(p=0.5),\r\n",
        "            A.VerticalFlip(p=0.1),\r\n",
        "            A.Normalize(\r\n",
        "                mean=[0.0, 0.0, 0.0],\r\n",
        "                std=[1.0, 1.0, 1.0],\r\n",
        "                max_pixel_value=255.0,\r\n",
        "            ),\r\n",
        "            ToTensorV2(),\r\n",
        "        ],\r\n",
        ")\r\n",
        "\r\n",
        "val_transforms = A.Compose(\r\n",
        "        [\r\n",
        "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\r\n",
        "            A.CenterCrop(224,224),\r\n",
        "            A.Normalize(\r\n",
        "                mean=[0.0, 0.0, 0.0],\r\n",
        "                std=[1.0, 1.0, 1.0],\r\n",
        "                max_pixel_value=255.0,\r\n",
        "            ),\r\n",
        "            ToTensorV2(),\r\n",
        "        ],\r\n",
        ")\r\n",
        "\r\n",
        "model = UNet(in_channels=3,\r\n",
        "             out_channels=1,\r\n",
        "             n_blocks=4,\r\n",
        "             start_filters=32,\r\n",
        "             activation='relu',\r\n",
        "             normalization='batch',\r\n",
        "             conv_mode='same',\r\n",
        "             dim=2).to(DEVICE)\r\n",
        "             #    model = UNET(in_channels=3, out_channels=1).to(DEVICE)\r\n",
        "loss_fn = nn.BCEWithLogitsLoss()\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\r\n",
        "\r\n",
        "train_loader, val_loader = get_loaders(\r\n",
        "        TRAIN_IMG_DIR,\r\n",
        "        TRAIN_MASK_DIR,\r\n",
        "        VAL_IMG_DIR,\r\n",
        "        VAL_MASK_DIR,\r\n",
        "        BATCH_SIZE,\r\n",
        "        train_transform,\r\n",
        "        val_transforms,\r\n",
        "        NUM_WORKERS,\r\n",
        "        PIN_MEMORY,\r\n",
        ")\r\n",
        "\r\n",
        "check_accuracy(val_loader, model, device=DEVICE)\r\n",
        "scaler = torch.cuda.amp.GradScaler()\r\n",
        "i = 1\r\n",
        "for epoch in range(NUM_EPOCHS):\r\n",
        "        print(f\"epoch: {i}\")\r\n",
        "        train_fn(train_loader, model, optimizer, loss_fn, scaler)\r\n",
        "        check_accuracy(val_loader, model, device=DEVICE)\r\n",
        "        i += 1\r\n",
        "        # print some examples to a folder\r\n",
        "save_predictions_as_imgs(\r\n",
        "            val_loader, model, folder=\"/content/drive/MyDrive/UNET/saved_images/\", device=DEVICE)\r\n",
        "\r\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 5570866/29654016 with acc 18.79\n",
            "Dice score: 0.31379058957099915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [10:16<00:00,  4.16s/it, loss=0.632]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 19971496/29654016 with acc 67.35\n",
            "Dice score: 0.39487674832344055\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:23<00:00,  6.39it/s, loss=0.592]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 21543526/29654016 with acc 72.65\n",
            "Dice score: 0.40692585706710815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:23<00:00,  6.31it/s, loss=0.559]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 21686464/29654016 with acc 73.13\n",
            "Dice score: 0.40970301628112793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:23<00:00,  6.21it/s, loss=0.557]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 22428580/29654016 with acc 75.63\n",
            "Dice score: 0.3869597315788269\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.06it/s, loss=0.517]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 23091791/29654016 with acc 77.87\n",
            "Dice score: 0.3317753076553345\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  5.98it/s, loss=0.502]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 23106134/29654016 with acc 77.92\n",
            "Dice score: 0.32364463806152344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.09it/s, loss=0.513]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 23335462/29654016 with acc 78.69\n",
            "Dice score: 0.33250877261161804\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.06it/s, loss=0.479]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 23837245/29654016 with acc 80.38\n",
            "Dice score: 0.16508176922798157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.05it/s, loss=0.51]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 23749490/29654016 with acc 80.09\n",
            "Dice score: 0.22295884788036346\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.04it/s, loss=0.44]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 23903889/29654016 with acc 80.61\n",
            "Dice score: 0.1322203427553177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.06it/s, loss=0.461]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24063420/29654016 with acc 81.15\n",
            "Dice score: 0.01734239049255848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.05it/s, loss=0.408]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24079623/29654016 with acc 81.20\n",
            "Dice score: 0.01867823302745819\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.05it/s, loss=0.537]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24077815/29654016 with acc 81.20\n",
            "Dice score: 0.02630545385181904\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.03it/s, loss=0.445]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24007355/29654016 with acc 80.96\n",
            "Dice score: 0.09307309240102768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.02it/s, loss=0.457]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24062895/29654016 with acc 81.15\n",
            "Dice score: 0.0783172994852066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.06it/s, loss=0.393]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24082322/29654016 with acc 81.21\n",
            "Dice score: 0.03335028141736984\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.06it/s, loss=0.398]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24064760/29654016 with acc 81.15\n",
            "Dice score: 0.034629158675670624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.06it/s, loss=0.489]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24081101/29654016 with acc 81.21\n",
            "Dice score: 0.020828576758503914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.05it/s, loss=0.401]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24095456/29654016 with acc 81.26\n",
            "Dice score: 0.009500766173005104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.06it/s, loss=0.344]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24012972/29654016 with acc 80.98\n",
            "Dice score: 0.10979615151882172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.06it/s, loss=0.376]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24091962/29654016 with acc 81.24\n",
            "Dice score: 0.01042048167437315\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.04it/s, loss=0.34]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24082203/29654016 with acc 81.21\n",
            "Dice score: 0.01468188688158989\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.09it/s, loss=0.457]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24066761/29654016 with acc 81.16\n",
            "Dice score: 0.07434126734733582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.02it/s, loss=0.396]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24099233/29654016 with acc 81.27\n",
            "Dice score: 0.004427524749189615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.06it/s, loss=0.49]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24083675/29654016 with acc 81.22\n",
            "Dice score: 0.05018097907304764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.04it/s, loss=0.457]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24097098/29654016 with acc 81.26\n",
            "Dice score: 0.002668283646926284\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.04it/s, loss=0.471]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24043358/29654016 with acc 81.08\n",
            "Dice score: 0.11599231511354446\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.07it/s, loss=0.397]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24084658/29654016 with acc 81.22\n",
            "Dice score: 0.05061180144548416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.07it/s, loss=0.335]\n",
            "  0%|          | 0/148 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24094739/29654016 with acc 81.25\n",
            "Dice score: 0.006468743085861206\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 148/148 [00:24<00:00,  6.07it/s, loss=0.335]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 24083160/29654016 with acc 81.21\n",
            "Dice score: 0.051084525883197784\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}